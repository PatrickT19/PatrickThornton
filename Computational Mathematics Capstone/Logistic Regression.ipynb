{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone in Mathematics Final Project\n",
    "## Logistic Regression -- Patrick Thornton\n",
    "## Not intended to be graded, just for those interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(feature):\n",
    "    x_data = pd.read_csv(feature)\n",
    "    x = x_data.values\n",
    "    return x\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns={'weathersit':'weather'},inplace=True)\n",
    "train = train.drop(['dteday'], axis=1)\n",
    "train['season'] = train.season.astype('category')\n",
    "train['holiday'] = train.holiday.astype('category')\n",
    "train['weather'] = train.weather.astype('category')\n",
    "\n",
    "test.rename(columns={'weathersit':'weather'},inplace=True)\n",
    "test = test.drop(['dteday'], axis=1)\n",
    "test['season'] = test.season.astype('category')\n",
    "test['holiday'] = test.holiday.astype('category')\n",
    "test['weather'] = test.weather.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoder\n",
    "### label is 0 -> [1 0 0 0 0 0 0 0 0]\n",
    "### label is 3 -> [0 0 0 1 0 0 0 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, column):       \n",
    "    df = pd.concat([df, pd.get_dummies(df[column], prefix=column, drop_first=True)],axis=1)\n",
    "    df = df.drop([column], axis=1)\n",
    "    return df\n",
    "\n",
    "train_ohe = train\n",
    "test_ohe = test\n",
    "\n",
    "cats = ['season','holiday','weather']\n",
    "for cat in cats:\n",
    "    train_ohe = one_hot_encoder(train_ohe, cat)\n",
    "    test_ohe = one_hot_encoder(test_ohe, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  train_ohe['cnt']\n",
    "X_train = train_ohe.drop(['cnt'], axis=1)\n",
    "y_test =  test_ohe['cnt']\n",
    "X_test = test_ohe.drop(['cnt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train, columns = ['cnt'])\n",
    "y_test = pd.DataFrame(y_test, columns = ['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    normalize the features\n",
    "    Using normal distribution\n",
    "'''\n",
    "def get_mean_variance(X):\n",
    "    mean = np.mean(X, axis=0) # axis=0: taking means along the\n",
    "    # vertical line (column)\n",
    "    # (sum(x_i-\\mu)^2)/N\n",
    "    X_temp = X - mean #\n",
    "    X_temp_entrypointwise = X_temp*X_temp\n",
    "    variance = np.mean(X_temp_entrypointwise, axis=0) #axis=0: \n",
    "    # taking means along the vertical line (column)\n",
    "    return mean, variance\n",
    "    \n",
    "def normalize_features(X_train, X_test):\n",
    "    mean, variance = get_mean_variance(X_train)\n",
    "    variance += 1e-15\n",
    "    ''' transform the feature '''\n",
    "    X_train_norm = (X_train - mean)/(np.sqrt(variance))\n",
    "    #math.sqrt doesnot work for numpy\n",
    "    X_test_norm = (X_test - mean)/np.sqrt(variance)\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "y_train = y_train.transform(lambda x: (x/250)).astype(int)\n",
    "y_test = y_test.transform(lambda x: (x/250)).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_one(X):\n",
    "    '''\n",
    "         convert X -> [1 X]\n",
    "    '''\n",
    "    # add  column of ones\n",
    "    ones = np.ones(X.shape[0])\n",
    "    ones = ones.reshape(-1, 1)\n",
    "    X_new = np.append(ones, X, axis=1)\n",
    "\n",
    "    return X_new\n",
    "\n",
    "X_train_norm_new = add_column_one(X_train_norm)\n",
    "X_test_norm_new = add_column_one(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(X, c):\n",
    "    ''' sigmoid function '''\n",
    "    return 1.0/(1.0 + np.exp(-X.dot(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_exact, y_pred):\n",
    "    return (-y_exact.T.dot(np.log(y_pred+1e-15))- (1.0 - y_exact).T.dot(np.log(1-y_pred+1e-15)))/float(len(y_exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, epochs=1000, learning_rate=0.0001):\n",
    "    '''\n",
    "        Input\n",
    "        -----\n",
    "        X: training features (normalized and having bias)\n",
    "        y: labels\n",
    "        \n",
    "        output\n",
    "        ------\n",
    "        c: optimal coeffs\n",
    "        loss_history\n",
    "    '''\n",
    "    loss_history = [0]*epochs\n",
    "    c_dim = X.shape[1]\n",
    "    n_samples = X.shape[0]\n",
    "    c = np.ones((c_dim, 1))\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predictor(X, c)\n",
    "        \n",
    "        loss_history[epoch] = loss(y_pred, y).ravel()[0] # (2D) (1,1) -> 1D\n",
    "                                                               # [5] -> 5\n",
    "       \n",
    "        XT = X.T\n",
    "        gradient = XT.dot(y_pred-y)/float(n_samples)   \n",
    "        # updating coeffs upon the gradient change\n",
    "        c = c - learning_rate*gradient\n",
    "    return c, loss_history\n",
    "\n",
    "def plot_loss(loss):\n",
    "    import matplotlib  # import package\n",
    "    import matplotlib.pyplot as plt # import library pytlot and change its name to plt\n",
    "    %matplotlib inline  \n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_label_encoder(y_train, y_test):\n",
    "    ''' convert label to a vector under one-hot-code fashion '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe\n",
    "# label is 0 -> [1 0 0 0 0 0 0 0 0]\n",
    "# label is 3 -> [0 0 0 1 0 0 0 0 0]\n",
    "y_train_ohe, y_test_ohe = one_label_encoder(y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi label train\n",
    "## One vs. the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    }
   ],
   "source": [
    "def multilabel_train(X, y):# y_train: one_hot_encoder labels\n",
    "    c_list = []\n",
    "    print(y.shape)\n",
    "    for i in range(y.shape[1]): # 3 columns \n",
    "        y_one_column = y[:, i].reshape(-1, 1) # pick ith columns\n",
    "        X = np.array(X,dtype=np.float32)\n",
    "        c_one_column, loss_history = gradient_descent(X, y_one_column, epochs=100, learning_rate=0.9)\n",
    "        #plot_loss(loss_history)\n",
    "        c_list.append(c_one_column)\n",
    "    return c_list\n",
    "c_list = multilabel_train(X_train_norm_new, y_train_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_prediction(c_list, X):\n",
    "    i = 0\n",
    "    X = np.array(X,dtype=np.float32)\n",
    "    for c in c_list:\n",
    "        probability = predictor(X, c)\n",
    "        # probabily of one column\n",
    "        if i == 0:\n",
    "            probability_matrix = probability\n",
    "        else:\n",
    "            # combine all decision columns to form a matrix\n",
    "            probability_matrix = np.concatenate(\n",
    "                              (probability_matrix, probability),\n",
    "                               axis=1)\n",
    "        i += 1\n",
    "    labels = []\n",
    "    for i in range(0,10000):\n",
    "        labels.append(i)\n",
    "    n_samples = X.shape[0]\n",
    "    # find which index gives us the highest probability\n",
    "    y_pred = np.zeros(n_samples, dtype=int)\n",
    "    for i in range(n_samples):\n",
    "        y_pred[i] = labels[np.argmax(probability_matrix[i,:])]\n",
    "    return y_pred\n",
    "\n",
    "y_pred = multilabel_prediction(c_list, X_test_norm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7220\n"
     ]
    }
   ],
   "source": [
    "def accuracy(ypred, yexact):\n",
    "    p = []\n",
    "    y = np.asarray(yexact)\n",
    "    for i in range(len(ypred)):\n",
    "        if ypred[i] == y[i][0]:\n",
    "            p.append(1)\n",
    "        else:\n",
    "            p.append(0)\n",
    "            \n",
    "    return np.sum(p)/float(len(yexact))\n",
    "print(\"Accuracy = %.4f\" % accuracy(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
